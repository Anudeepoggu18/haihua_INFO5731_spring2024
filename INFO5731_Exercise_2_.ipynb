{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DymRJbxDBCnf"
      },
      "source": [
        "# **INFO5731 In-class Exercise 2**\n",
        "\n",
        "The purpose of this exercise is to understand users' information needs, and then collect data from different sources for analysis by implementing web scraping using Python.\n",
        "\n",
        "**Expectations**:\n",
        "*   Students are expected to complete the exercise during lecture period to meet the active participation criteria of the course.\n",
        "*   Use the provided .*ipynb* document to write your code & respond to the questions. Avoid generating a new file.\n",
        "*   Write complete answers and run all the cells before submission.\n",
        "*   Make sure the submission is \"clean\"; *i.e.*, no unnecessary code cells.\n",
        "*   Once finished, allow shared rights from top right corner (*see Canvas for details*).\n",
        "\n",
        "**Total points**: 40\n",
        "\n",
        "**Deadline**: This in-class exercise is due at the end of the day tomorrow, at 11:59 PM.\n",
        "\n",
        "**Late submissions will have a penalty of 10% of the marks for each day of late submission. , and no requests will be answered. Manage your time accordingly.**\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Question 1 (10 Points)\n",
        "Describe an interesting research question (or practical question or something innovative) you have in mind, what kind of data should be collected to answer the question(s)? Specify the amount of data needed for analysis. Provide detailed steps for collecting and saving the data."
      ],
      "metadata": {
        "id": "FBKvD6O_TY6e"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# write your answer here\n",
        "Problem statement : Evaluating Blood Quality for the donation in order to Enhance protection for the People.\n",
        "\n",
        "A thorough set of information encasing different facets of blood elements, recipient traits, conditions of preservation,\n",
        "testing processes, and clinical results should be gathered in order to evaluate the standard of fluid for a blood donation.\n",
        "The size of the investigation and the quantity of specifications necessitate will determine how much data is required.\n",
        "In order to monitor modifications across a period of time and evaluate the cumulative impact of platelet transplants\n",
        "on the health of patients, information ought to be gathered over time.\n",
        "Since the data relates to the medical field, reliable data cannot be found on data collecting websites.\n",
        "The appropriate parties approval and authorization must be obtained.\n",
        "Verify for biases and anomalies in the information after it has been collected. After cleaning, preserve the information\n",
        "that is collected. It is vital to take into account the feasibility of gathering and organizing substantial amounts of\n",
        "information while guaranteeing the accuracy of the sample. Furthermore, anonymity and security should be protected at all times\n",
        "during the information gathering procedure, and it ought to be legal. In order to ascertain what size of sample is necessary for\n",
        "significant results according to the expected impact size and information variation, we should perform a significant assessment.\n",
        "Relatively few datasets, however, are available to the general public for use in a quick study or as references in\n",
        "previously published papers.\n"
      ],
      "metadata": {
        "id": "klqHgdtYQ4D_"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Question 2 (10 Points)\n",
        "Write Python code to collect a dataset of 1000 samples related to the question discussed in Question 1."
      ],
      "metadata": {
        "id": "E9RqrlwdTfvl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import random\n",
        "\n",
        "def quality_of_blood(samples=1000):\n",
        "    data = []\n",
        "    for _ in range(samples):\n",
        "        try:\n",
        "            # Features of elements in blood\n",
        "            rbc_count = random.uniform(4, 6)\n",
        "            wbc_count = random.uniform(4, 10)\n",
        "            platelet_count = random.uniform(150000, 400000)\n",
        "            hemoglobin = random.uniform(12, 18)\n",
        "            clotting_factor = random.uniform(10, 15)\n",
        "\n",
        "            # Conditions for carrying and storing\n",
        "            Temperature = random.uniform(1, 10)\n",
        "            Duration = random.randint(1, 30)\n",
        "            transportation_method = random.choice(['Air', 'Ground', 'Sea'])\n",
        "\n",
        "            # Recipient data\n",
        "            Age = random.randint(18, 65)\n",
        "            Health_status = random.choice(['Good', 'Average', 'Poor'])\n",
        "            Factors = random.choice(['Active', 'Moderate', 'Inactive'])\n",
        "\n",
        "            # Findings of the viral infection testing\n",
        "            HIV = random.choice(['Negative', 'Positive'])\n",
        "            Hepatitis_b = random.choice(['Negative', 'Positive'])\n",
        "            Hepatitis_c = random.choice(['Negative', 'Positive'])\n",
        "            Syphilis = random.choice(['Negative', 'Positive'])\n",
        "\n",
        "            # Results\n",
        "            Post_transfusion = random.choice(['Yes', 'No'])\n",
        "            Infection = random.choice(['Yes', 'No'])\n",
        "            Health = random.choice(['Improved', 'Stable', 'Deteriorated'])\n",
        "\n",
        "            # Data dictionary\n",
        "            Data = {\n",
        "                'Red_Blood_Cell_Count': rbc_count,\n",
        "                'White_Blood_Cell_Count': wbc_count,\n",
        "                'Platelet_Count': platelet_count,\n",
        "                'Hemoglobin_Levels': hemoglobin,\n",
        "                'Clotting_Factors': clotting_factor,\n",
        "                'Storage_Temperature': Temperature,\n",
        "                'Storage_Duration': Duration,\n",
        "                'Transportation_Method': transportation_method,\n",
        "                'Donor_Age': Age,\n",
        "                'Donor_Health_Status': Health_status,\n",
        "                'Donor_Lifestyle_Factor': Factors,\n",
        "                'HIV_Status': HIV,\n",
        "                'Hepatitis_B_Status': Hepatitis_b,\n",
        "                'Hepatitis_C_Status': Hepatitis_c,\n",
        "                'Syphilis_Status': Syphilis,\n",
        "                'Post_Transfusion_Reaction': Post_transfusion,\n",
        "                'Infection_Status': Infection,\n",
        "                'Overall_Patient_Health': Health\n",
        "            }\n",
        "\n",
        "            data.append(Data)\n",
        "        except Exception as e:\n",
        "            print(f\"Error occurred: {e}\")\n",
        "\n",
        "    return pd.DataFrame(data)\n",
        "\n",
        "try:\n",
        "    # Generating the dataset with 1000 samples\n",
        "    Dataset = quality_of_blood(samples=1000)\n",
        "\n",
        "    # Saving the data to a CSV file\n",
        "    Dataset.to_csv('Bloodquality_Data.csv', index=False)\n",
        "    print(\"CSV file saved successfully!\")\n",
        "except Exception as e:\n",
        "    print(f\"Error occurred: {e}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VXGb57nQpKzU",
        "outputId": "19705823-952a-4821-d1a4-763634f0f791"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CSV file saved successfully!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(Dataset.head())\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7Nc-01FQppKx",
        "outputId": "8feef5a7-f0f9-444c-a2f5-e32533f705be"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "   Red_Blood_Cell_Count  White_Blood_Cell_Count  Platelet_Count  \\\n",
            "0              4.072087                8.269328   167904.044384   \n",
            "1              5.257697                5.153952   374862.246504   \n",
            "2              4.250449                7.792254   209821.818530   \n",
            "3              4.562664                4.268163   386681.252569   \n",
            "4              4.731505                9.915149   173090.214581   \n",
            "\n",
            "   Hemoglobin_Levels  Clotting_Factors  Storage_Temperature  Storage_Duration  \\\n",
            "0          16.568495         12.743822             9.104866                10   \n",
            "1          15.776842         10.233523             3.834656                 4   \n",
            "2          16.685722         11.047452             2.354377                10   \n",
            "3          16.459936         12.047180             9.939225                13   \n",
            "4          16.157157         12.494141             6.098342                15   \n",
            "\n",
            "  Transportation_Method  Donor_Age Donor_Health_Status Donor_Lifestyle_Factor  \\\n",
            "0                   Sea         34                Good                 Active   \n",
            "1                   Air         52             Average                 Active   \n",
            "2                Ground         31                Poor                 Active   \n",
            "3                Ground         65             Average               Inactive   \n",
            "4                Ground         33             Average                 Active   \n",
            "\n",
            "  HIV_Status Hepatitis_B_Status Hepatitis_C_Status Syphilis_Status  \\\n",
            "0   Negative           Negative           Positive        Negative   \n",
            "1   Negative           Negative           Positive        Negative   \n",
            "2   Negative           Negative           Positive        Negative   \n",
            "3   Positive           Positive           Positive        Negative   \n",
            "4   Positive           Negative           Positive        Positive   \n",
            "\n",
            "  Post_Transfusion_Reaction Infection_Status Overall_Patient_Health  \n",
            "0                       Yes              Yes           Deteriorated  \n",
            "1                       Yes               No               Improved  \n",
            "2                        No               No               Improved  \n",
            "3                       Yes               No               Improved  \n",
            "4                       Yes              Yes               Improved  \n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "03jb4GZsBkBS"
      },
      "source": [
        "## Question 3 (10 Points)\n",
        "Write Python code to collect 1000 articles from Google Scholar (https://scholar.google.com/), Microsoft Academic (https://academic.microsoft.com/home), or CiteSeerX (https://citeseerx.ist.psu.edu/index), or Semantic Scholar (https://www.semanticscholar.org/), or ACM Digital Libraries (https://dl.acm.org/) with the keyword \"XYZ\". The articles should be published in the last 10 years (2014-2024).\n",
        "\n",
        "The following information from the article needs to be collected:\n",
        "\n",
        "(1) Title of the article\n",
        "\n",
        "(2) Venue/journal/conference being published\n",
        "\n",
        "(3) Year\n",
        "\n",
        "(4) Authors\n",
        "\n",
        "(5) Abstract"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YaGLbSHHB8Ej",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ec4e3319-f7b4-45ff-808b-39d34c05fcb7"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (2.31.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests) (3.6)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests) (2024.2.2)\n",
            "Title: Title not found\n",
            "Venue: Venue not found\n",
            "Year: Year not found\n",
            "Authors: Authors not found\n",
            "Abstract: Abstract not found\n",
            "\n",
            "---\n",
            "\n",
            "Title: Title not found\n",
            "Venue: Venue not found\n",
            "Year: Year not found\n",
            "Authors: Authors not found\n",
            "Abstract: Abstract not found\n",
            "\n",
            "---\n",
            "\n",
            "Title: Title not found\n",
            "Venue: Venue not found\n",
            "Year: Year not found\n",
            "Authors: Authors not found\n",
            "Abstract: \n",
            "\n",
            "---\n",
            "\n",
            "Title: Title not found\n",
            "Venue: Venue not found\n",
            "Year: Year not found\n",
            "Authors: Authors not found\n",
            "Abstract: Semantic Scholar uses groundbreaking AI and engineering to understand the semantics of scientific literature to help Scholars discover relevant research.\n",
            "\n",
            "---\n",
            "\n",
            "Title: Title not found\n",
            "Venue: Venue not found\n",
            "Year: Year not found\n",
            "Authors: Authors not found\n",
            "Abstract: ACM Digital Library Home page\n",
            "\n",
            "---\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# write yo!pip install requests beautifulsoup4\n",
        "!pip install requests\n",
        "import requests\n",
        "from bs4 import BeautifulSoup\n",
        "\n",
        "def get_article_info(url):\n",
        "    response = requests.get(url)\n",
        "    soup = BeautifulSoup(response.text, 'html.parser')\n",
        "\n",
        "    title_tag = soup.find('meta', {'name': 'citation_title'})\n",
        "    title = title_tag.get('content') if title_tag else \"Title not found\"\n",
        "\n",
        "    venue_tag = soup.find('meta', {'name': 'citation_conference_title'})\n",
        "    venue = venue_tag.get('content') if venue_tag else \"Venue not found\"\n",
        "\n",
        "    year_tag = soup.find('meta', {'name': 'citation_publication_date'})\n",
        "    year = year_tag.get('content') if year_tag else \"Year not found\"\n",
        "\n",
        "    authors_tag = soup.find('meta', {'name': 'citation_authors'})\n",
        "    authors = authors_tag.get('content') if authors_tag else \"Authors not found\"\n",
        "\n",
        "    abstract_tag = soup.find('meta', {'name': 'description'})\n",
        "    abstract = abstract_tag.get('content') if abstract_tag else \"Abstract not found\"\n",
        "\n",
        "    return {\n",
        "        'Title': title,\n",
        "        'Venue': venue,\n",
        "        'Year': year,\n",
        "        'Authors': authors,\n",
        "        'Abstract': abstract\n",
        "    }\n",
        "\n",
        "\n",
        "# List of URLs\n",
        "urls = [\"https://scholar.google.com/\", \"https://academic.microsoft.com/home\", \"https://citeseerx.ist.psu.edu/index\",\"https://www.semanticscholar.org/\",\"https://dl.acm.org/\"]\n",
        "      # Collect articles\n",
        "articles = [get_article_info(url) for url in urls]\n",
        "for article in articles:\n",
        "      print(f\"Title: {article['Title']}\")\n",
        "      print(f\"Venue: {article['Venue']}\")\n",
        "      print(f\"Year: {article['Year']}\")\n",
        "      print(f\"Authors: {article['Authors']}\")\n",
        "      print(f\"Abstract: {article['Abstract']}\")\n",
        "      print(\"\\n---\\n\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Scraping the data using semantic scholar"
      ],
      "metadata": {
        "id": "ZUbyeaCw9RlP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import requests\n",
        "import json\n",
        "import pandas as pd\n",
        "\n",
        "# Define your API key\n",
        "api_key = \"aCHeM6vVJf2B9K8bBPlY5UvSbyJ0bUY4Y0zXasqe\"\n",
        "\n",
        "# Define the topic you want to search for\n",
        "topic = 'covid'\n",
        "\n",
        "# Construct the URL with the topic, API key, and desired fields\n",
        "url = f\"https://api.semanticscholar.org/graph/v1/paper/search?query={topic}&offset=100&limit=3&fields=title,abstract,authors\"\n",
        "\n",
        "# Make the GET request to the API\n",
        "response = requests.get(url)\n",
        "\n",
        "# Check if the request was successful\n",
        "if response.status_code == 200:\n",
        "    # Convert the response text to JSON\n",
        "    response_json = response.json()\n",
        "\n",
        "    # Print the entire response to understand its structure\n",
        "    print(\"Response:\", response_json)\n",
        "\n",
        "    # Check if the response contains the key 'data'\n",
        "    if 'data' in response_json:\n",
        "        # Extract the 'data' key from the response\n",
        "        data = response_json['data']\n",
        "\n",
        "        # Create a pandas DataFrame from the extracted data\n",
        "        df = pd.DataFrame(data)\n",
        "\n",
        "        # Display the DataFrame\n",
        "        print(df.head())\n",
        "    else:\n",
        "        print(\"No 'data' key found in the response.\")\n",
        "else:\n",
        "    print(\"Error:\", response.status_code)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JHGg023O0NYZ",
        "outputId": "dec3d0b8-91db-428c-a685-20a51e44d503"
      },
      "execution_count": 52,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Error: 429\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jJDe71iLB616"
      },
      "source": [
        "## Question 4A (10 Points)\n",
        "Develop Python code to collect data from social media platforms like Reddit, Instagram, Twitter (formerly known as X), Facebook, or any other. Use hashtags, keywords, usernames, or user IDs to gather the data.\n",
        "\n",
        "\n",
        "\n",
        "Ensure that the collected data has more than four columns.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MtKskTzbCLaU"
      },
      "outputs": [],
      "source": [
        "# write your answer here\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "55W9AMdXCSpV"
      },
      "source": [
        "## Question 4B (10 Points)\n",
        "If you encounter challenges with Question-4 web scraping using Python, employ any online tools such as ParseHub or Octoparse for data extraction. Introduce the selected tool, outline the steps for web scraping, and showcase the final output in formats like CSV or Excel.\n",
        "\n",
        "\n",
        "\n",
        "Upload a document (Word or PDF File) in any shared storage (preferably UNT OneDrive) and add the publicly accessible link in the below code cell.\n",
        "\n",
        "Please only choose one option for question 4. If you do both options, we will grade only the first one"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from bs4 import BeautifulSoup\n",
        "import requests\n",
        "import pandas as pd"
      ],
      "metadata": {
        "id": "sqt3RUAFU4nn"
      },
      "execution_count": 59,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "url = \"https://www.imdb.com/chart/toptv/?ref_=nv_tvv_250\"\n",
        "page = requests.get(url)\n",
        "page\n",
        "soup = BeautifulSoup(page.content, \"html.parser\")"
      ],
      "metadata": {
        "id": "MADGLTnmVMkT"
      },
      "execution_count": 61,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# scrap tv show names\n",
        "scraped_TVshows = soup.find_all('td', class_='titleColumn')\n",
        "scraped_TVshows"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bYl__S6_VgKG",
        "outputId": "050b75db-408a-4ab8-86ba-9cd3f0cef7f7"
      },
      "execution_count": 63,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[]"
            ]
          },
          "metadata": {},
          "execution_count": 63
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# parsing the tv show names\n",
        "TVshows = []\n",
        "for TVshow in scraped_TVshows:\n",
        "    TVshow = TVshow.get_text().replace('\\n', \"\")\n",
        "    TVshow = TVshow.strip(\" \")\n",
        "    TVshow = TVshow.split('.')[1]\n",
        "    TVshows.append(TVshow)\n",
        "TVshows"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tlDD4BeUVTiW",
        "outputId": "6966ce53-c877-4dc4-eaef-cdeb15fe726b"
      },
      "execution_count": 64,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[]"
            ]
          },
          "metadata": {},
          "execution_count": 64
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Mandatory Question"
      ],
      "metadata": {
        "id": "sZOhks1dXWEe"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Important: Reflective Feedback on Web Scraping and Data Collection**\n",
        "\n",
        "\n",
        "\n",
        "Please share your thoughts and feedback on the web scraping and data collection exercises you have completed in this assignment. Consider the following points in your response:\n",
        "\n",
        "\n",
        "\n",
        "Learning Experience: Describe your overall learning experience in working on web scraping tasks. What were the key concepts or techniques you found most beneficial in understanding the process of extracting data from various online sources?\n",
        "\n",
        "\n",
        "\n",
        "Challenges Encountered: Were there specific difficulties in collecting data from certain websites, and how did you overcome them? If you opted for the non-coding option, share your experience with the chosen tool.\n",
        "\n",
        "\n",
        "\n",
        "Relevance to Your Field of Study: How might the ability to gather and analyze data from online sources enhance your work or research?\n",
        "\n",
        "**(no grading of your submission if this question is left unanswered)**"
      ],
      "metadata": {
        "id": "eqmHVEwaWhbV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "'''\n",
        "Write your response here.\n",
        "Web scraping is a new topic for me. I learned new techniques used for web scraping. It is an interesting topic and can be used for data collection.\n",
        " I still have few doubts but I am eager to learn all the other ways of scraping data. I had difficulty in collecting data from social media websites and I have requested an API key\n",
        " but due to insufficient time I opted for alternative task.\n",
        " Relevance to Your Field of Study: Data scraping can be used in different projects for collecting or extracting data. So I hope I can use this technique in my future research works\n",
        "'''"
      ],
      "metadata": {
        "id": "akAVJn9YBTQT",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 106
        },
        "outputId": "43b044eb-2dba-4f2d-ea3d-1b75ef643d19"
      },
      "execution_count": 47,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'\\nWrite your response here.\\nWeb scraping is a new topic for me. I learned new techniques used for web scraping. It is an interesting topic and can be used for data collection.\\n I still have few doubts but I am eager to learn all the other ways of scraping data. I had difficulty in collecting data from social media websites and I have requested an API key \\n but due to insufficient time I opted for alternative task.\\n Relevance to Your Field of Study: Data scraping can be used in different projects for collecting or extracting data. So I hope I can use this technique in my future research works \\n'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 47
        }
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": [
        "FBKvD6O_TY6e",
        "E9RqrlwdTfvl",
        "03jb4GZsBkBS",
        "jJDe71iLB616",
        "55W9AMdXCSpV",
        "4ulBZ6yhCi9F",
        "6SmvS7nSfbj8",
        "sZOhks1dXWEe"
      ],
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.12"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}